# AxonFlow Runtime Configuration
# =================================
# This file configures MCP connectors and LLM providers for OSS/self-hosted deployments.
#
# Usage:
#   1. Copy this file to one of the following locations:
#      - ./axonflow.yaml (current directory)
#      - ./config/axonflow.yaml
#      - /etc/axonflow/axonflow.yaml
#   2. Or set AXONFLOW_CONFIG_FILE=/path/to/your/config.yaml
#
# Environment Variables:
#   - References: ${VAR_NAME} or $VAR_NAME
#   - Default values: ${VAR_NAME:-default_value}
#
# Configuration Priority (highest to lowest):
#   1. Database (Enterprise - via Customer Portal)
#   2. Config File (OSS - this file)
#   3. Environment Variables (Legacy)

version: "1.0"

# ==============================================================================
# MCP CONNECTORS
# ==============================================================================
# Configure connections to external data sources (databases, APIs, etc.)
# Connectors enable AI agents to access your data safely with policy enforcement.
#
# Supported types: postgres, cassandra, salesforce, amadeus, slack, snowflake, custom

connectors:
  # --------------------------------------------------------------------------
  # PostgreSQL Connector Example
  # --------------------------------------------------------------------------
  # Enables SQL queries against PostgreSQL databases with automatic
  # dangerous operation blocking (DROP, DELETE, TRUNCATE, etc.)
  postgres_main:
    type: postgres
    enabled: true
    display_name: "Main Application Database"
    description: "Primary PostgreSQL database for application data"

    # Connection URL - supports env var expansion
    connection_url: ${DATABASE_URL:-postgres://localhost:5432/myapp}

    # Credentials (prefer env vars for sensitive data)
    credentials:
      username: ${POSTGRES_USER:-postgres}
      password: ${POSTGRES_PASSWORD}

    # Connection pool and behavior options
    options:
      max_open_conns: 25           # Maximum open connections
      max_idle_conns: 5            # Maximum idle connections
      conn_max_lifetime: "5m"      # Connection max lifetime
      schema: "public"             # Default schema
      ssl_mode: "require"          # SSL mode: disable, require, verify-full

    # Timeout and retry settings
    timeout_ms: 30000              # Query timeout in milliseconds
    max_retries: 3                 # Number of retry attempts

  # --------------------------------------------------------------------------
  # Cassandra Connector Example
  # --------------------------------------------------------------------------
  # Enables CQL queries against Cassandra/ScyllaDB clusters
  cassandra_cluster:
    type: cassandra
    enabled: false                 # Enable when configured
    display_name: "Analytics Cluster"
    description: "Cassandra cluster for time-series analytics"

    credentials:
      hosts: ${CASSANDRA_HOSTS:-localhost}
      port: ${CASSANDRA_PORT:-9042}
      username: ${CASSANDRA_USER}
      password: ${CASSANDRA_PASSWORD}
      keyspace: ${CASSANDRA_KEYSPACE:-analytics}

    options:
      consistency: "QUORUM"        # Consistency level
      dc: "datacenter1"            # Data center name

    timeout_ms: 30000

  # --------------------------------------------------------------------------
  # Snowflake Connector Example
  # --------------------------------------------------------------------------
  # Enables SQL queries against Snowflake data warehouses
  snowflake_warehouse:
    type: snowflake
    enabled: false                 # Enable when configured
    display_name: "Snowflake Data Warehouse"
    description: "Enterprise data warehouse for analytics"

    credentials:
      account: ${SNOWFLAKE_ACCOUNT}
      username: ${SNOWFLAKE_USER}
      password: ${SNOWFLAKE_PASSWORD}
      # Or use key-pair authentication:
      # private_key_path: ${SNOWFLAKE_PRIVATE_KEY_PATH}

    options:
      warehouse: ${SNOWFLAKE_WAREHOUSE:-COMPUTE_WH}
      database: ${SNOWFLAKE_DATABASE}
      schema: ${SNOWFLAKE_SCHEMA:-PUBLIC}
      role: ${SNOWFLAKE_ROLE:-PUBLIC}

    timeout_ms: 60000              # Longer timeout for warehouse queries

  # --------------------------------------------------------------------------
  # Salesforce Connector Example
  # --------------------------------------------------------------------------
  # Enables SOQL queries and object operations against Salesforce
  salesforce_crm:
    type: salesforce
    enabled: false                 # Enable when configured
    display_name: "Salesforce CRM"
    description: "Customer relationship management data"

    credentials:
      client_id: ${SALESFORCE_CLIENT_ID}
      client_secret: ${SALESFORCE_CLIENT_SECRET}
      username: ${SALESFORCE_USERNAME}
      password: ${SALESFORCE_PASSWORD}
      security_token: ${SALESFORCE_SECURITY_TOKEN}

    options:
      instance_url: ${SALESFORCE_INSTANCE_URL:-https://login.salesforce.com}
      api_version: "58.0"

    timeout_ms: 30000

  # --------------------------------------------------------------------------
  # Slack Connector Example
  # --------------------------------------------------------------------------
  # Enables Slack channel and message operations
  slack_workspace:
    type: slack
    enabled: false                 # Enable when configured
    display_name: "Company Slack"
    description: "Slack workspace for team communication"

    credentials:
      bot_token: ${SLACK_BOT_TOKEN}
      app_token: ${SLACK_APP_TOKEN}

    options:
      default_channel: ${SLACK_DEFAULT_CHANNEL:-general}

    timeout_ms: 10000

  # --------------------------------------------------------------------------
  # Amadeus Travel API Connector Example
  # --------------------------------------------------------------------------
  # Enables travel search operations (flights, hotels, etc.)
  amadeus_travel:
    type: amadeus
    enabled: false                 # Enable when configured
    display_name: "Amadeus Travel API"
    description: "Travel booking and search API"

    credentials:
      api_key: ${AMADEUS_API_KEY}
      api_secret: ${AMADEUS_API_SECRET}

    options:
      environment: ${AMADEUS_ENV:-test}  # test or production
      cache_enabled: true
      cache_ttl: "15m"

    timeout_ms: 30000

# ==============================================================================
# LLM PROVIDERS
# ==============================================================================
# Configure Large Language Model providers for AI processing.
# Multiple providers can be configured for load balancing and failover.
#
# Supported providers: bedrock, ollama, openai, anthropic
#
# Routing Strategy:
#   - Priority: Higher value = preferred (used for failover ordering)
#   - Weight: 0.0-1.0 (used for load distribution among same-priority providers)

llm_providers:
  # --------------------------------------------------------------------------
  # Amazon Bedrock (Recommended for AWS deployments)
  # --------------------------------------------------------------------------
  # Uses AWS IAM for authentication - no API key required if running on AWS
  bedrock:
    enabled: true
    display_name: "Amazon Bedrock"

    config:
      region: ${AWS_REGION:-us-east-1}
      model: ${BEDROCK_MODEL:-anthropic.claude-3-5-sonnet-20240620-v1:0}
      # Available models:
      # - anthropic.claude-3-5-sonnet-20240620-v1:0 (recommended)
      # - anthropic.claude-3-haiku-20240307-v1:0 (faster, cheaper)
      # - meta.llama3-70b-instruct-v1:0

    # Routing configuration
    priority: 10                   # Highest priority - primary provider
    weight: 0.8                    # 80% of traffic when all providers healthy

  # --------------------------------------------------------------------------
  # Ollama (Self-hosted, good for local/private deployments)
  # --------------------------------------------------------------------------
  # Run your own LLMs on-premises with no external API calls
  ollama:
    enabled: false                 # Enable when running Ollama locally
    display_name: "Ollama (Self-hosted)"

    config:
      endpoint: ${OLLAMA_ENDPOINT:-http://localhost:11434}
      model: ${OLLAMA_MODEL:-llama3.1:70b}
      # Available models depend on what you've pulled:
      # - llama3.1:70b (best quality, requires ~40GB RAM)
      # - llama3.1:8b (good balance)
      # - codellama:34b (code-focused)

    priority: 5                    # Failover priority
    weight: 0.2                    # 20% of traffic

  # --------------------------------------------------------------------------
  # OpenAI (Alternative commercial provider)
  # --------------------------------------------------------------------------
  openai:
    enabled: false                 # Enable when API key is available
    display_name: "OpenAI"

    config:
      model: ${OPENAI_MODEL:-gpt-4-turbo}
      max_tokens: 4096
      temperature: 0.7

    credentials:
      api_key: ${OPENAI_API_KEY}

    priority: 5
    weight: 0.5

  # --------------------------------------------------------------------------
  # Anthropic Direct API (Alternative to Bedrock)
  # --------------------------------------------------------------------------
  anthropic:
    enabled: false                 # Enable when API key is available
    display_name: "Anthropic Direct"

    config:
      model: ${ANTHROPIC_MODEL:-claude-3-5-sonnet-20241022}
      max_tokens: 8192

    credentials:
      api_key: ${ANTHROPIC_API_KEY}

    priority: 5
    weight: 0.5
