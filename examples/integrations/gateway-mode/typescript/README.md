# Gateway Mode - TypeScript Example

**Lowest latency AI governance** - Add compliance and monitoring to your LLM calls with ~3-5ms overhead.

## What is Gateway Mode?

Gateway Mode separates policy enforcement from LLM calls, giving you:

1. **Full control** - Use any LLM provider directly (OpenAI, Anthropic, etc.)
2. **Lowest latency** - Policy checks happen in parallel, ~3-5ms overhead
3. **Complete audit trails** - Every interaction logged for compliance
4. **Flexibility** - Customize LLM parameters, streaming, etc.

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Your App   â”‚â”€â”€â”€â”€â”€â–¶â”‚  AxonFlow   â”‚â”€â”€â”€â”€â”€â–¶â”‚  Audit DB   â”‚
â”‚             â”‚      â”‚  (Pre-check)â”‚      â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                         â–²
       â”‚                                         â”‚
       â–¼                                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM API    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  AxonFlow   â”‚
â”‚  (OpenAI)   â”‚        Response           â”‚  (Audit)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Quick Start

### Prerequisites

- Node.js 18+
- Docker (for local AxonFlow)
- OpenAI or Anthropic API key

### 1. Start AxonFlow

```bash
# From the axonflow repo root
docker compose up -d
```

### 2. Install Dependencies

```bash
npm install
```

### 3. Configure Environment

```bash
cp .env.example .env
# Edit .env with your API keys
```

### 4. Run the Example

```bash
# OpenAI example
npm run start:openai

# Anthropic Claude example
npm run start:anthropic

# Default (OpenAI)
npm start
```

## Expected Output

```
ğŸš€ AxonFlow Gateway Mode - TypeScript Example

ğŸ“¤ Query: "What are the benefits of AI governance in enterprise?"
ğŸ‘¤ User: user-123
ğŸ“‹ Context: { user_role: 'developer', department: 'engineering' }

â±ï¸  Step 1: Policy Pre-Check...
   âœ… Pre-check completed in 3ms
   ğŸ“ Context ID: abc123-def456
   âœ“ Approved: true

ğŸ¤– Step 2: LLM Call (OpenAI)...
   âœ… LLM response received in 850ms
   ğŸ“Š Tokens: 25 prompt, 150 completion

ğŸ“ Step 3: Audit Logging...
   âœ… Audit logged in 2ms

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š Results
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ’¬ Response:
AI governance in enterprise provides several key benefits...

â±ï¸  Latency Breakdown:
   Pre-check:  3ms
   LLM call:   850ms
   Audit:      2ms
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Governance: 5ms (overhead)
   Total:      855ms
```

## Code Walkthrough

### Step 1: Pre-Check

```typescript
const preCheckResult = await axonflow.getPolicyApprovedContext({
  userToken,
  query,
  context,
});

if (!preCheckResult.approved) {
  console.log(`Blocked: ${preCheckResult.blockReason}`);
  return;
}
```

The pre-check validates:
- SQL injection patterns
- PII in the query (SSN, credit cards, etc.)
- User permissions (RBAC)
- Rate limits
- Custom policies

### Step 2: LLM Call

```typescript
const completion = await openai.chat.completions.create({
  model: "gpt-3.5-turbo",
  messages: [{ role: "user", content: query }],
});
```

You make the LLM call directly - full control over:
- Model selection
- Parameters (temperature, max_tokens, etc.)
- Streaming
- Retries

### Step 3: Audit

```typescript
await axonflow.auditLLMCall({
  contextId: preCheckResult.contextId,
  response: response,
  model: "gpt-3.5-turbo",
  provider: "openai",
  tokenUsage: { ... },
  latencyMs: llmLatency,
});
```

The audit logs:
- Request/response correlation
- Token usage for cost tracking
- Latency metrics
- Provider information

## Policy Enforcement Examples

### Blocked - SQL Injection

```typescript
const query = "SELECT * FROM users; DROP TABLE users;";
// Result: blocked, reason: "SQL injection attempt detected"
```

### Blocked - PII Detection

```typescript
const query = "Process payment for SSN 123-45-6789";
// Result: blocked, reason: "US Social Security Number pattern detected"
```

### Allowed - Safe Query

```typescript
const query = "What is the weather forecast?";
// Result: approved: true
```

## Files

```
typescript/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index.ts           # Main OpenAI example
â”‚   â””â”€â”€ anthropic-example.ts  # Anthropic Claude example
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

## Next Steps

- [Proxy Mode](../proxy-mode/typescript/) - Simpler integration, AxonFlow handles LLM routing
- [LangChain Integration](../../langchain/) - Use with LangChain framework
- [Policy Management](../../../policies/crud/) - Create custom policies

## Troubleshooting

### Connection Refused

```
Error: connect ECONNREFUSED 127.0.0.1:8080
```

**Solution:** Start AxonFlow with `docker compose up -d`

### Invalid API Key

```
Error: OpenAI API key is invalid
```

**Solution:** Check your `.env` file has a valid `OPENAI_API_KEY`

### Pre-check Blocked

If requests are unexpectedly blocked, check:
1. Query content for PII patterns
2. SQL-like syntax in the query
3. User role permissions in context
